{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Categorical Crossentropy Loss Function\n",
    "This loss function is used when you want to classify your data into different categories.\n",
    "\n",
    "A good exmaple of this would be recognising different animals in an image - e.g. Cat, Dog, Horse, Cow, Sheep.\n",
    "\n",
    "In this notebook we'll be extending the work from Categorical Crossentropy Loss Function notebook to use the Sparse Categorical Crossentropy loss function. This loss function lets us avoid one hot encoding our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard set of imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants for use later\n",
    "image_width = 100\n",
    "image_height = 100\n",
    "epochs = 3\n",
    "steps_per_epoch = 100\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "We're going to generate some dummy training data. We'll generate some simple images that are blank, contain a square or a circle or a triangle. Our model will try and predict what class the image falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"blank\",\"square\",\"circle\",\"triangle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "This is an infinite generator that will generate a pair of input and outpus values.\n",
    "\n",
    "As we are using the `SparseCategoricalCrossentropy` loss function we no longer need to one-hot encode our label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    i = 0\n",
    "    while(True):\n",
    "        if i >= 1000:\n",
    "            i = 0\n",
    "        # our output value will be the one hot encoded version of: 0,1,2,3 - corresponding to our labels - \"blank\", \"square\", \"circle\", \"triangle\"\n",
    "        Y = i % 4\n",
    "        X = np.zeros((image_width, image_height, 1))\n",
    "        \n",
    "        # size of our shape\n",
    "        radius = int(np.random.uniform(10,20))\n",
    "        # position of our shape\n",
    "        center_x = int(np.random.uniform(radius, image_width - radius))\n",
    "        center_y = int(np.random.uniform(radius, image_height - radius))\n",
    "        \n",
    "        if Y == 1: # generate a square\n",
    "            X[center_y - radius:center_y + radius, center_x - radius:center_x + radius] = 1        \n",
    "        elif Y == 2: # generate a circle\n",
    "            for y in range(-radius, radius):\n",
    "                for x in range(-radius, radius):\n",
    "                    if x*x + y*y <= radius*radius:\n",
    "                        X[y+center_y, x+center_x] = 1\n",
    "        elif Y==3:\n",
    "            for y in range(-radius, radius):\n",
    "                for x in range(-radius, radius):\n",
    "                    if abs(x) < (y+radius)/2:\n",
    "                        X[y+center_y, x+center_x] = 1            \n",
    "        else: # blank image\n",
    "            pass\n",
    "        yield X, [Y]\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "This creates a dataset that we can feed into the tensorflow fit function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator, \n",
    "    output_types = (tf.float32, tf.int32),\n",
    "    output_shapes=((image_width, image_height, 1), (1))\n",
    ")\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising our training data\n",
    "We can display a selection of images from our training data along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_images(X, Y):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "    axes = axes.flatten()\n",
    "    for x, y, ax in zip(X, tf.cast(Y, tf.float32), axes):\n",
    "        # show the image\n",
    "        ax.imshow(x)\n",
    "        # work out the label\n",
    "        label = labels[int(y)]\n",
    "        # show the label beneath the plot\n",
    "        ax.text(10, image_height+10, f\"{label}\", bbox={'facecolor': 'white', 'pad': 5})\n",
    "        # turn off the axis to make the display a bit cleaner\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# get a batch of images and labels from our dataset\n",
    "X, Y = next(iter(train_dataset))\n",
    "# and show the images along with the label - not the use of nump() to get the values from the tensor\n",
    "plot_images(X, tf.squeeze(Y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our very simple mode\n",
    "We don't need a very complicated model for our problem, so we'll just define a small convolutional neural network with a hidden layer and an output layer. Our output layer now has 4 neurons as we have 4 different classes to predict.\n",
    "\n",
    "It's important that the activation function should be `softmax`. This activation function will make sure that the output values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(8, 3, \n",
    "           padding='same',\n",
    "           activation='relu',\n",
    "           input_shape=(image_width, image_height, 1),\n",
    "           name='conv_layer'),\n",
    "    MaxPooling2D(name='max_pooling'),\n",
    "    Flatten(),\n",
    "    Dense(\n",
    "        10,\n",
    "        activation='relu',\n",
    "        name='hidden_layer'\n",
    "    ),\n",
    "    Dense(4, activation='softmax', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile our model\n",
    "For our loss function we need to use `SparseCategoricalCrossentropy`.\n",
    "\n",
    "Crossentropy quantifies the difference between two probability distribution.\n",
    "\n",
    "We have more than two classes so need to use CategoricalCrossentropy for our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting our model\n",
    "We can now fit our model to the data. We have a very simple model to solve so we don't need to run for very many epochs to get a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our model with some examples\n",
    "You should end up with an accuracy approaching > 90%. Longer training should give you something approaching 100%.\n",
    "\n",
    "Let's generate some more examples and see what the output of the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of samples from the dataset\n",
    "X, _ = next(iter(train_dataset))\n",
    "# ask the model to predict the output for our samples\n",
    "predicted_Y = model.predict(X.numpy())\n",
    "# work out the max indices for each sample\n",
    "max_indexes = tf.argmax(predicted_Y, axis=1)\n",
    "# show the images along with the predicted value\n",
    "plot_images(X, max_indexes)\n",
    "# set the format to 2 decimal places\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "predicted_Y[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Did your model work? Some interesting experiments to try:\n",
    "* How small can you make your model before it fails?\n",
    "* What happens if you feed a completely different shape into the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
